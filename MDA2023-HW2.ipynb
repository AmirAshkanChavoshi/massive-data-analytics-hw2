{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262a1420",
   "metadata": {},
   "source": [
    "# Data Mining Course Spark Exercise\n",
    "## Sharif University of Technology\n",
    "\n",
    "In this notebook we are going to analyze farsi news. \n",
    "Outline of the exercise:\n",
    "* Dataset preparation\n",
    "* Preprocessing \n",
    "* Exploration \n",
    "* Word Collections\n",
    "\n",
    "You should replace the `TODO` parts with your implementation. Remeber that each `TODO` may take multiple lines and you shouldn't limit your self to one-line codes.\n",
    "\n",
    "## Prerequisites\n",
    "You should be faimilar with [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). In this notebook you should use the following formula for tf-idf:\n",
    "$$f_{t,d}/len(d) \\times log(1 + \\frac{N}{n_t})$$\n",
    "\n",
    "## Warning: RDD api only\n",
    "You **can not** use Dataframe, Dataset, mllib, ml, ... apis of spark in this exercise. You should only use the [RDD api](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75576da1",
   "metadata": {},
   "source": [
    "# Please enter your name below:\n",
    "# Name:\n",
    "# Student Number:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d038ab",
   "metadata": {},
   "source": [
    "# Section 1: Dataset prepartition\n",
    "This section you need to download [dataset](https://drive.google.com/file/d/1bRxHQDzPr6wDimbM7b89H47kH8O3YV8Y/view?usp=sharing) in a directory you work. After that run the below cell to untar the datase.\n",
    "\n",
    "**Note 1: Don't change the below command.**\n",
    "\n",
    "**Note 2: If you use Windows OS, unzip the dataset manually.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0ae4f",
   "metadata": {},
   "source": [
    "## Install Pypark & Initialization\n",
    "Uncomment this section if you use google colab or local pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237c3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3706a8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/30 23:11:15 WARN Utils: Your hostname, aac resolves to a loopback address: 127.0.1.1; using 192.168.98.134 instead (on interface ens33)\n",
      "23/12/30 23:11:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/30 23:11:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/30 23:11:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"HW2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"30g\") \\\n",
    "    .config(\"spark.executor.memory\", \"30g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", 0) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc:SparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13157b6b",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e395d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling 1/20 of the data\n",
    "news_rdd = sc.textFile(\"news_data.jsonl\") \\\n",
    ".sample(False, 1/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bad10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to json objects\n",
    "import json\n",
    "json_rdd = news_rdd.map(lambda x: json.loads(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056f0d5",
   "metadata": {},
   "source": [
    "# Section 2: Preprocessing\n",
    "This section we try to normalize news and remove useless characters (for example /n and /u200c and ...). Also find and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a24d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless characters\n",
    "import re\n",
    "def remove_useless_characters(x):\n",
    "    x['body'] = re.sub(r'[^آ-ی ]', \"\", x['body'])\n",
    "    return x\n",
    "clean_news_rdd = json_rdd.map(lambda x: remove_useless_characters(x)) #TODO: remove useless charachters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86644fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding and removing stopwords.\n",
    "def remove_stop_words(x):\n",
    "    stop_words = [\"اعلام\", \"انحام\", \"قرار\", \"مورد\", \"باید\", \"دارد\", \"میشود\", \"کرده\", \"خواهد\", \"داد\", \"دو\", \"و\", \"کرد\", \"شده\", \"گفت\", \"در\", \"به\", \"از\", \"که\", \"این\", \"را\", \"با\", \"های\", \"برای\", \"تا\", \"ها\", \"است\", \"یا\", \"اما\", \"هم\", \"نه\", \"یک\",\n",
    "    \"خود\", \"یکی\", \"بود\", \"شد\", \"می\", \"شود\", \"کرد\", \"او\", \"ما\", \"شما\", \"آن\", \"آنها\", \"ایشان\", \"با\", \"در\", \"بر\", \"برای\",\n",
    "    \"به\", \"روی\", \"زیر\", \"بالا\", \"جلوی\", \"پشت\", \"پنجره\", \"دنبال\", \"جلو\", \"پیش\", \"پس\", \"نزدیک\", \"دور\", \"برخی\", \"چند\",\n",
    "    \"چنین\", \"چه\", \"چون\", \"چگونه\", \"کجا\", \"کدام\", \"که\", \"کدامین\", \"کدامیک\", \"کدامیکی\", \"کجا\", \"کی\", \"چرا\", \"چطور\", \"چندین\",\n",
    "    \"چند\", \"چنین\", \"کسی\", \"کس\", \"کدامیک\", \"کدامین\", \"کدامیکی\", \"کدام\", \"اینکه\", \"آنکه\", \"ولی\", \"اما\", \"اگر\", \"هر\", \"هرکس\",\n",
    "    \"هرچه\", \"همان\", \"تنها\", \"وقتی\", \"تا\", \"تازه\", \"الان\", \"همیشه\", \"هنوز\", \"همین\", \"اول\", \"حالا\", \"پیش\", \"پس\", \"بار\",\n",
    "    \"بارها\", \"همچنین\", \"نه\", \"نیز\", \"بلکه\", \"بله\", \"بلکه\", \"بلکه\", \"آره\", \"آری\", \"آره\", \"آری\", \"آره\", \"آری\", \"آره\",\n",
    "    \"آری\", \"هم\", \"دیگر\", \"هم\", \"دیگران\", \"دیگری\", \"دیگه\", \"دیگه\", \"دیگران\", \"دیگری\", \"جلو\", \"پیش\", \"پس\", \"بالا\", \"پایین\"]\n",
    "    x['body'] = \" \".join([word for word in x['body'].split() if word not in stop_words])\n",
    "    return x\n",
    "processed_news_rdd = clean_news_rdd.map(lambda x: remove_stop_words(x)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cc032",
   "metadata": {},
   "source": [
    "<b>\n",
    "<h1> Section2: Assign number to each document </h1>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "743e3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generating a new ID for each document\n",
    "def generate_new_id(x):\n",
    "    x['new_id'] = ids.index(x['uid'])  \n",
    "    return x  \n",
    "ids = processed_news_rdd.sortBy(lambda x: x['uid']).map(lambda x: x['uid']).collect()\n",
    "numbered_news_rdd = processed_news_rdd.map(generate_new_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3cb55d",
   "metadata": {},
   "source": [
    "<b>\n",
    "<h1> Section3: Shingling </h1>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3e23ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generating the list of all shingles\n",
    "size_one_shingle = 7\n",
    "def partition_string(x):\n",
    "    body = x['body']\n",
    "    shingle_presence = [x['new_id']]\n",
    "    body_shingles = []\n",
    "    for i in range(len(body)-size_one_shingle):\n",
    "        body_shingles.append((str(body[i:i+size_one_shingle]), shingle_presence))\n",
    "    return body_shingles\n",
    "# Merging shingle presence lists with the same key(shingle)\n",
    "shingle_doc_matrix = numbered_news_rdd.flatMap(partition_string).reduceByKey(lambda x,y: x+y).sortByKey()\n",
    "total_number_of_shingles = len(shingle_doc_matrix.collect())\n",
    "all_shingles_rdd = shingle_doc_matrix.map(lambda x: x[0])\n",
    "all_shingles_list = all_shingles_rdd.collect()\n",
    "all_shingles_dict = {value: index for index, value in enumerate(all_shingles_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8bcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a new RDD where keys represent the document's new IDs, \n",
    "# and values represent the IDs of the shingles it contains.\n",
    "def add_shingle_id(x):\n",
    "    return (x[1], all_shingles_dict[x[0]]) # Keys are document new ids and values are shingle numbers.\n",
    "def break_keys(x):\n",
    "    return [(element, x[1]) for element in x[0]]\n",
    "def process_group(x):\n",
    "    return (x[0], list(x[1]))\n",
    "shingle_doc_matrix_with_shingle_id = shingle_doc_matrix.map(add_shingle_id) \\\n",
    ".flatMap(break_keys) \\\n",
    ".groupByKey() \\\n",
    ".map(process_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5bb19",
   "metadata": {},
   "source": [
    "<b>\n",
    "<h1> Section4: Generating Hash Functions </h1>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6dd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating prime numbers larger than total_ingnumber_of_shingles\n",
    "\n",
    "from sympy import primerange\n",
    "\n",
    "def generate_primes_larger_than(start):\n",
    "    \"\"\"Generate prime numbers larger than a specified number using sympy.\"\"\"\n",
    "    return primerange(start + 1, start + 100)  # Adjust the range as needed\n",
    "\n",
    "specific_number = total_number_of_shingles\n",
    "primes = list(generate_primes_larger_than(specific_number))\n",
    "\n",
    "P = primes[0]  # The first prime number after total number of shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968ce12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating hash functions\n",
    "import random\n",
    "number_of_hash_functions = 300\n",
    "def generate_hash_function(N, P):\n",
    "\n",
    "    a = random.randint(1, P - 1)  # a should be in the range [1, P-1]\n",
    "    b = random.randint(0, P - 1)  # b can be any value in the range [0, P-1]\n",
    "\n",
    "    return (a, b)\n",
    "\n",
    "hash_functions_coefficients = [generate_hash_function(total_number_of_shingles, P) for i in range(number_of_hash_functions)]\n",
    "# Hash function numbers start from 1\n",
    "hash_functions_coefficients = [(i+1, hash_functions_coefficients[i]) for i in range(number_of_hash_functions)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920e749",
   "metadata": {},
   "source": [
    "# Section 5: Generating signature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac1ccb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the hash for each document\n",
    "def apply_hash(x):\n",
    "    output_list = []\n",
    "    hash_values_list = []\n",
    "    for i in range(len(x[1])):\n",
    "        hash_values_list = []\n",
    "        for a, b in hash_functions_coefficients:\n",
    "            hash_values_list.append((a, (((b[0]*x[1][i]+b[1])%P)%total_number_of_shingles)))\n",
    "        output_list.append((x[1][i], hash_values_list))\n",
    "    return (x[0], output_list)\n",
    "        \n",
    "hashed_shigle_doc_matrix = shingle_doc_matrix_with_shingle_id.map(apply_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b55c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding minimum hash (min hashgin)\n",
    "def find_min_hash(x):\n",
    "    hash_values = []\n",
    "    min_hash = [total_number_of_shingles] * number_of_hash_functions\n",
    "    min_hash_index = [0] * number_of_hash_functions\n",
    "    \n",
    "    for item in x[1]:\n",
    "        for sub_item in item[1]:\n",
    "            if sub_item[1] < min_hash[sub_item[0]-1]:\n",
    "                min_hash[sub_item[0]-1] = sub_item[1]\n",
    "            \n",
    "\n",
    "    return (x[0], min_hash)\n",
    "    \n",
    "        \n",
    "signature_matrix  = hashed_shigle_doc_matrix.map(find_min_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaabb9f",
   "metadata": {},
   "source": [
    "# Section 6: Generating condidate similar pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df2bfee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Converting the signature matrix into bands and rows and generating similar pairs\n",
    "b = 60\n",
    "r = number_of_hash_functions/b\n",
    "def divid_into_bounds(x):\n",
    "    return_list = []\n",
    "    counter = 0 # Shows the bound number (starting from 0)\n",
    "    for i in range(0, number_of_hash_functions, int(r)):    \n",
    "        subtuple = tuple(x[1][i:i + int(r)])    \n",
    "        return_list.append(((counter, subtuple), x[0]))\n",
    "        counter = counter + 1\n",
    "    return return_list\n",
    "\n",
    "def generate_condidates(x):\n",
    "    return (x[0], tuple(x[1]))\n",
    "\n",
    "bounded_signature_matrix = signature_matrix.flatMap(divid_into_bounds) \\\n",
    ".groupByKey() \\\n",
    ".map(generate_condidates)\n",
    "condidate_similar_list = bounded_signature_matrix.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a007efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "condidate_similar_news = []\n",
    "for pair in condidate_similar_list:\n",
    "    if len(pair[1]) > 1:\n",
    "        condidate_similar_news.append(pair[1]) \n",
    "numbered_news = numbered_news_rdd.collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7ffb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing jacard similarity metric for each pair of condidates\n",
    "total_number_of_condidates = 0\n",
    "def compute_jacard_similarity(doc_new_id1, doc_new_id2):\n",
    "    flag1 = 0\n",
    "    flag2 = 0\n",
    "    for new in numbered_news:\n",
    "        if new['new_id'] == doc_new_id1:\n",
    "            body1 = new['body']\n",
    "            flag1 = 1\n",
    "        if new['new_id'] == doc_new_id2:\n",
    "            body2 = new['body']\n",
    "            flag2 = 1\n",
    "        if flag1 ==1 and flag2 == 1:\n",
    "            break\n",
    "\n",
    "    body1_shingles = []\n",
    "    body2_shingles = []\n",
    "    for i in range(len(body1)-size_one_shingle):\n",
    "        body1_shingles.append(str(body1[i:i+size_one_shingle]))\n",
    "    for i in range(len(body2)-size_one_shingle):\n",
    "        body2_shingles.append(str(body2[i:i+size_one_shingle]))\n",
    "    common_shingles = set(body1_shingles).intersection(set(body2_shingles))\n",
    "    union_shingles = set(body1_shingles).union(set(body2_shingles))\n",
    "    return len(common_shingles)/len(union_shingles)\n",
    "# Uncomment the following lines to find all similar items.\n",
    "\n",
    "# for item in condidate_similar_news:\n",
    "#     if len(item) > 2:\n",
    "#         jacard_similarity = compute_jacard_similarity(item[0], item[1])\n",
    "#         if jacard_similarity > 0.8:\n",
    "#             print(f\"Condidate item is {item}. Jacard similarity is {jacard_similarity}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "504b3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The uid of your document is 2922f433fc511183774e409f3\n",
      "\n",
      "The title of your document is: بازگشت ستاره آبی‌ها به ترکیب اصلی استقلال\n",
      "\n",
      "The url of your document is: http://9sobh.news/fa/news/55831\n",
      "\n",
      "The body of your docuoment is:\n",
      "بازیکن استقلال میتواند تیم ادامه بازیهای لیگ برتر فوتبال همراهی کندبه گزارش صبح آرمین سهرابیان طور کامل بند مصدومیت رهایی پیدا میتواند تیم فوتبال استقلال تهران ادامه بازیهای لیگ برتر همراهی کندآرمین سهرابیان بعد بازی صنعت نفت دچار مصدومیت پایان دوران نقاهت آماده حضور ترکیب استقلال صورت صلاحدید جواد نکونام میتواند بازی آلومینیوم اراک ترکیب استقلال میدان بیایداستقلال بازیهای اخیر سه مدافع میانی بازی شاید حضور دوباره آرمین سهرابیان ترکیب تیم آبی روزبه چشمی خط هافبک برگردداستقلال بازی هوادار روزبه چشمی محمد حسین مرادمند ایمان سلیمی بازی آنجایی استقلال شیوه مدافع بازیهای اخیر امتیازات مهمی دست آورده بینی برابر آلومینیوم اراک شیوه بازی کند\n",
      "--------------------------------------------------------------------\n",
      "d220a968de71a64b7bfabe12c is similar to your document and the jacard similarity is 0.805\n",
      "\n",
      "The title of this condidate is: استقلال از پرسپولیس جلو زد - ساناپرس\n",
      "\n",
      "The url of this condidate is: https://sanapress.ir/319970/%D8%A7%D8%B3%D8%AA%D9%82%D9%84%D8%A7%D9%84-%D8%A7%D8%B2-%D9%BE%D8%B1%D8%B3%D9%BE%D9%88%D9%84%DB%8C%D8%B3-%D8%AC%D9%84%D9%88-%D8%B2%D8%AF/\n",
      "\n",
      "The body of this condidate is:\n",
      "گزارش ساناپرس استقلال پرسپولیس زدآرمین سهرابیان طور کامل بند مصدومیت رهایی پیدا میتواند استقلال ادامه بازیهای لیگ برتر همراهی کندآرمین سهرابیان بعد بازی صنعت نفت دچار مصدومیت پایان دوران نقاهت آماده حضور ترکیب استقلال صورت صلاحدید جواد نکونام میتواند بازی آلومینیوم اراک ترکیب استقلال میدان بیایداستقلال بازیهای اخیر سه مدافع میانی بازی شاید حضور دوباره آرمین سهرابیان ترکیب تیم آبی روزبه چشمی خط هافبک برگردداستقلال بازی هوادار روزبه چشمی محمد حسین مرادمند ایمان سلیمی بازی آنجایی استقلال شیوه مدافع بازیهای اخیر امتیازات مهمی دست آورده بینی برابر آلومینیوم اراک شیوه بازی کند\n",
      "--------------------------------------------------------------------\n",
      "4cc7df2e9c017ce61b4318465 is similar to your document and the jacard similarity is 0.850\n",
      "\n",
      "The title of this condidate is: بازیکن مصدوم استقلال آماده حضور در ترکیب تیم اصلی\n",
      "\n",
      "The url of this condidate is: http://aftabnews.ir/fa/news/869312\n",
      "\n",
      "The body of this condidate is:\n",
      "آرمین سهرابیان طور کامل بند مصدومیت رهایی پیدا میتواند استقلال ادامه بازیهای لیگ برتر همراهی کندآرمین سهرابیان بعد بازی صنعت نفت دچار مصدومیت پایان دوران نقاهت آماده حضور ترکیب استقلال صورت صلاحدید جواد نکونام میتواند بازی آلومینیوم اراک ترکیب استقلال میدان بیایداستقلال بازیهای اخیر سه مدافع میانی بازی شاید حضور دوباره آرمین سهرابیان ترکیب تیم آبی روزبه چشمی خط هافبک برگردداستقلال بازی هوادار روزبه چشمی محمد حسین مرادمند ایمان سلیمی بازی آنجایی استقلال شیوه مدافع بازیهای اخیر امتیازات مهمی دست آورده بینی برابر آلومینیوم اراک شیوه بازی کند\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def convert_uid_to_new_id(uid):\n",
    "    for new in numbered_news:\n",
    "        if new['uid'] == uid:\n",
    "            new_id = new['new_id']\n",
    "            return new, new_id\n",
    "    print(\"No document was found!\") \n",
    "    return None\n",
    "\n",
    "def convert_new_id_to_uid(new_id):\n",
    "    for new in numbered_news:\n",
    "        if new['new_id'] == new_id:\n",
    "            uid = new['uid']\n",
    "            return new, uid\n",
    "        \n",
    "    return None      \n",
    "\n",
    "uid = input(\"Please enter the uid:\")\n",
    "similarity_lower_bound = input(\"Please enther the lower bound of similarity for finding condidates:\")\n",
    "counter = 0\n",
    "main_new, new_id = convert_uid_to_new_id(uid)\n",
    "similar_items = []\n",
    "flag = 0\n",
    "for item in condidate_similar_news:\n",
    "    if new_id in item and len(item) > 1:\n",
    "        if flag == 0:\n",
    "            print (f\"The uid of your document is {uid}\")\n",
    "            print(f\"\\nThe title of your document is: {main_new['title']}\")\n",
    "            print(f\"\\nThe url of your document is: {main_new['url']}\")\n",
    "            print(f\"\\nThe body of your docuoment is:\")\n",
    "            print(main_new['body'])\n",
    "            print(\"--------------------------------------------------------------------\")\n",
    "        flag = 1\n",
    "        while counter < len(item):\n",
    "            if item[counter] != new_id and compute_jacard_similarity(new_id, item[counter]) > float(similarity_lower_bound):\n",
    "                new, similar_item = convert_new_id_to_uid(item[counter])\n",
    "                similar_items.append(similar_items)\n",
    "                print(f\"{similar_item} is similar to your document and the jacard similarity is {compute_jacard_similarity(new_id, item[counter]):.3f}\")\n",
    "                print(f\"\\nThe title of this condidate is: {new['title']}\")\n",
    "                print(f\"\\nThe url of this condidate is: {new['url']}\")\n",
    "                print(f\"\\nThe body of this condidate is:\")\n",
    "                print(new['body'])\n",
    "                print(\"--------------------------------------------------------------------\")\n",
    "            counter = counter + 1\n",
    "if flag == 0:\n",
    "    print(\"No similar documents were found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
